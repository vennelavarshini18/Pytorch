{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f16b57fe",
   "metadata": {},
   "source": [
    "BACK PROPAGATION USING PYTORCH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3bcf1e",
   "metadata": {},
   "source": [
    "y=(x^n)\n",
    "dy/dx = n*(x^(n-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2a88e3",
   "metadata": {},
   "source": [
    "Step 1: Forward Propagation (Making Predictions)\n",
    "\n",
    "Input Data: You start with input data — like an image, text, or numbers.\n",
    "\n",
    "Passing Through the Network: The data goes through each layer of the neural network.\n",
    "\n",
    "Weighted Sum: In each neuron (node), inputs are multiplied by weights and then added together with a bias.\n",
    "\n",
    "Activation Function: This sum passes through an activation function (like ReLU or sigmoid) to add non-linearity.\n",
    "\n",
    "Output: Eventually, the data reaches the output layer, which produces predictions (like class probabilities or numeric values)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5995e9",
   "metadata": {},
   "source": [
    "Step 2: Calculate Error\n",
    "\n",
    "Loss Function: Compare the network’s prediction to the true answer using a loss function (e.g., mean squared error or cross-entropy).\n",
    "\n",
    "Error Value: This gives a number representing how “wrong” the prediction is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e136cf66",
   "metadata": {},
   "source": [
    "Step 3: Backward Propagation (Learning from Mistakes)\n",
    "\n",
    "Calculate Gradients: The network calculates how much each weight and bias contributed to the error using calculus (specifically, derivatives).\n",
    "\n",
    "Chain Rule: It applies the chain rule to find gradients layer by layer, moving backward from the output towards the input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fa6ed9",
   "metadata": {},
   "source": [
    "Step 4: Update Weights\n",
    "\n",
    "Adjust Weights and Biases: Using the gradients, the network updates weights and biases to reduce error (usually with an optimization method like Gradient Descent).\n",
    "\n",
    "Repeat: This process (forward pass, error calculation, backward pass, weight update) repeats many times with different data until the model learns well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c5d5fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22690089",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=torch.tensor(4.0,requires_grad=True)    #Basically says that gradient should be intialised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6f2e036",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4., requires_grad=True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f7486f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=x**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34924940",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(16., grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d781106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Back propagartion ==> y=2*x\n",
    "y.backward()      #Computes the gradient of the current tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fd50560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8.)\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfeecb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assume as Neural network flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84e645a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst=[[2.,3.,1.],[4.,5.,3.],[7.,6.,4.]]    #Floating point numbers only\n",
    "torch_input=torch.tensor(lst,requires_grad=True)   #X "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57f02b91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 3., 1.],\n",
       "        [4., 5., 3.],\n",
       "        [7., 6., 4.]], requires_grad=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54ce3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example function (y=x**3+x**2)\n",
    "## derivative=3x**2+2x\n",
    "y=torch_input**3+torch_input**2 #Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a01193ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 12.,  36.,   2.],\n",
       "        [ 80., 150.,  36.],\n",
       "        [392., 252.,  80.]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5182793",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Output of layer=Z\n",
    "z=y.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3da46966",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1040., grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a139ce96",
   "metadata": {},
   "outputs": [],
   "source": [
    "z.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce42196f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 16.,  33.,   5.],\n",
       "        [ 56.,  85.,  33.],\n",
       "        [161., 120.,  56.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now backpropagartion\n",
    "torch_input.grad   #Applies derivative directly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc08182e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can calculate loss and then do  other steps now"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
